---
title: "HR Data Analysis Example"
author: "James Carr"
date: "10/17/2021"
output: 
  html_document:
    toc: true
    toc_float: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(DT)
library(knitr)
library(plotly)
```
### Introduction to the data

I found three government workforce data sets online from different public websites and downloaded them in *.csv format. You can view these data sets for yourself in my [github repo](https://github.com/jrcarr83/HR-example/tree/main/data). Unfortunately, these are not relational databases so joins do not make much sense; however, they do contain similar data types and columns so unions will be useful. I really enjoy using RStudio and Markdown, and I will be using dplyr from the tidyverse package to do most of my analytics, but when possible, I will also provide the SQL equivalent code.

```{r import_data, echo=FALSE, message=FALSE, warning=FALSE}
bloomington <- read_csv('data/bloomington.csv') %>%
                mutate(names = factor(employee),
                       departments = factor(department),
                       salaries = `annual rate`) %>%
                       select(-`annual rate`, -department, -employee)
chicago <- read_csv('data/chicago.csv') %>%
            mutate(names = factor(Name),
                   job_titles = factor(`Job Titles`),
                   departments = factor(Department),
                   ft_or_pt = factor(`Full or Part-Time`),
                   wage_types = factor(`Salary or Hourly`),
                   hours = `Typical Hours`,
                   salaries = `Annual Salary`,
                   hourly_rates = `Hourly Rate`) %>%
                   select(-`Name`, -`Job Titles`, -Department,
                          -`Full or Part-Time`, -`Salary or Hourly`,
                          -`Typical Hours`, -`Annual Salary`,
                          -`Hourly Rate`)
louisville <- read_csv('data/louisville.csv') %>%
              mutate(hrs_types = factor(WORKED_HRS_TYPE),
                     div_names = factor(DivName),
                     departments = factor(Dept),
                     years = factor(FiscalYear),
                     amt_hours = AMOUNT_HOURS,
                     amt_dollars = AMOUNT_DOLLARS) %>%
              select(-L_ACCT, -L_FUND, -WORKED_HRS_TYPE, -L_DIVISION,
                     -DivName, -DEPTID, -Dept, -SubDept, 
                     -BIWEEKLY_PAY_PERIOD_END_DATE, -FiscalYear, -EARN_CODE,
                     -EARN_DESCR, -AMOUNT_HOURS, -AMOUNT_DOLLARS)
marin <- read_csv('data/marin.csv') 
marin_cols <- names(marin)
marin <- marin %>%
           mutate(person_nums = `Open Data Person Number`,
                  genders = factor(Gender),
                  ethnicities = factor(`Ethnic Origin`),
                  origins = factor(`Regular Hire Origin`),
                  hire_dates = `Regular Hire Date`,
                  termination_dates = `Termination Date`,
                  termination_reasons = factor(`Termination Reason`),
                  data_dates = `Point in Time`,
                  active_fg = factor(`Active Employee (on date)`),
                  age_ranges = factor(`Age Range (on date)`),
                  job_desc = factor(`SOC Job Group Description (on date)`),
                  departments = factor(`Department (on date)`),
                  years = `Calendar Year`) %>%
            select(-marin_cols)
```

I tried to keep a consistent naming convention across all of the data sets - lower case and separated by underscores instead of spaces. It seems that each data set contains a department variable and some sort of salary or wage value, which will make for a good comparison across the cities. The Marin data contains the most descriptive columns, allowing for more interesting analysis. Louisville is the least like the others: it looks at summarized values by department rather than individual. Let's look at the departments:

### Duplicate rows for individuals
In two of the data sets, employees occassionaly have duplicate rows. I would imagine this could happen due to changes in role, salary, or other status type changes. The SQL code to find these individuals would be:

**Sample SQL to check for duplicate names**
```
select names, count(*) as num_rows
from bloomington
group by names
having num_rows > 1
```
```{r bloom_depts_dupes, echo=FALSE, warning=FALSE, message=FALSE}
#number of rows with duplicate names
num_rows <- nrow(bloomington %>% group_by(names, departments) %>% 
             tally() %>% filter(n > 1))
tbl <- cbind(unname(num_rows))
kable(tbl, caption='Number of Duplicated Names')
```

It would make sense to take the maximum value for each person, instead of having the duplicate rows. 

**Sample SQL to remove duplicate names**
```
select names, departments, max(salaries) as salaries
from bloomington
group by names, departments
```

```{r bloom_depts_dedupe, echo=FALSE, warning=FALSE, message=FALSE}
#group by names and departments and max the salaries
bloomington <- bloomington %>% group_by(names, departments) %>% 
                summarise(salaries = max(salaries))
#check that there is only one row per person per department now
num_rows<- nrow(bloomington %>% group_by(names, departments) %>% 
             tally() %>% filter(n > 1))
tbl <- cbind(unname(num_rows))
kable(tbl, caption='Number of Duplicated Names - Bloomington post dedupe')
```
After grouping the data by the max for each individual, there were no more duplicates.

A similar check was run for Chicago, and there were no duplicates.
```{r chic_dupes, echo=FALSE, message=FALSE, warnings=FALSE}
#number of rows with duplicate names
num_rows <- nrow(chicago %>% group_by(names, job_titles, departments,
                             wage_types, ft_or_pt) %>% 
             tally() %>% filter(n > 1))
tbl <- cbind(unname(num_rows))
kable(tbl, caption='Number of Duplicated Names Chicago')
```
Chicago has 40 individuals with multiple rows in a department with the same title, employment type, etc. Again, these probably people who received some sort of salary change. Taking the max salary seems appropriate:
```
select names
      ,job_titles
      ,departments
      ,wage_types
      ,ft_or_pt
      ,max(hours) as hours
      ,max(salaries) as salaries
      ,max(hourly_rates) as hourly_rates
from chicago
group by names
      ,job_titles
      ,departments
      ,wage_types
      ,ft_or_pt
```
```{r chic_dedupe, warning=FALSE, message=FALSE, echo=FALSE}
chicago <- chicago %>% group_by(names, job_titles, departments,
                             wage_types, ft_or_pt) %>%
                    summarise(hours = max(hours),
                              salaries = max(salaries),
                              hourly_rates = max(hourly_rates))

num_rows <- nrow(chicago %>% group_by(names, job_titles, departments,
                             wage_types, ft_or_pt) %>% 
             tally() %>% filter(n > 1))
tbl <- cbind(unname(num_rows))
kable(tbl, caption='Number of Duplicated Names Chicago')
```
Having resolved the duplicates in Chicago, we can now move one to Marin.

Finally, Marin is a multi-year data set. We should check for duplicates within those years and determine the last date in the date range.
```{r marin_dupes, echo=FALSE, message=FALSE, warnings=FALSE}
#marin is a multi-year dataset, are any duplicates in the same year?
num_rows <- nrow(marin %>% group_by(person_nums, years) %>% 
             tally() %>% filter(n > 1))
tbl <- cbind(unname(num_rows))
kable(tbl, caption='Number of Duplicated Names - Marin')
#no duplicates but to compare vs other cities, will have to limit to 1 year
#what's the max year?
max_year <- marin %>% summarise(max_year = max(years))
kable(max_year, caption='Max year in Marin Data')
```

### Summarizing the data
The data sets are all slightly different, so each will have to be summarized in their own way.

#### Bloomington
Bloomington is a spare data set. It only consists of names, departments, and salaries. 
```{r bloom_sum, warning=FALSE, message=FALSE, echo=FALSE}
bloom_sum <- bloomington %>% group_by(departments) %>% 
                summarise(`min` = min(salaries),
                          `25%` = quantile(salaries, 0.25),
                          `median` = quantile(salaries, 0.5),
                          `mean` = mean(salaries),
                          `75%` = quantile(salaries, 0.75),
                          `max` = max(salaries))
datatable(bloom_sum) %>% 
    formatCurrency(c('min', '25%', 'mean', 'median',
                                          '75%', 'max'), digits=0)
```

Looking at the summary table, there are clearly some part-time positions that skew the data leftwards. Lots of these appear to be administrative positions that could be flagged manually by department. To save time, I am going to flag them by the 20% quantile.
```{r bloom_quant, warning=FALSE, message=FALSE, echo=FALSE}
#grab 20% quantile
cutoff <- quantile(bloomington$salaries, 0.25)
#get density 
curve <- density(bloomington$salaries)
#put density into tibble and filter to cutoff
df <- tibble(data.frame(cbind(x=curve$x,y=curve$y))) %>%
      filter(between(x, min(bloomington$salaries), cutoff))

#plot
p <- ggplot(data=bloomington, aes(x=salaries)) +
      geom_density() + 
      geom_ribbon(data=df, aes(x, ymin=0, ymax=y)) +
      theme_classic() +
      ggtitle('Density of Salaries in Bloomington')
p
```

Looking at the graph above, the 20% quantile is near $25,000, and we'll flag part-time at that location and then rerun our summary tables.
```{r bloom_resum, echo=FALSE, warning=FALSE, message=FALSE}
bloomington <- bloomington %>%
                mutate(emp_type = if_else(
                  salaries >= 25000, 'Full Time', 'Part Time'))

bloom_sum <- bloomington %>% group_by(departments, emp_type) %>% 
                summarise(`min` = min(salaries),
                          `25%` = quantile(salaries, 0.25),
                          `median` = quantile(salaries, 0.5),
                          `mean` = mean(salaries),
                          `75%` = quantile(salaries, 0.75),
                          `max` = max(salaries))
datatable(bloom_sum) %>% 
    formatCurrency(c('min', '25%', 'mean', 'median',
                                          '75%', 'max'), digits=0)
                         
```

#### Chicago
One of the first things that is noticeable about Chicago's data, is that we have salary and hourly workers. I am going to multiply the hours worked by the hourly rate (times 52 weeks) and put it into the salary column to get it all into one place.
```{r chicago_sum, echo=FALSE, warning=FALSE, message=FALSE}
chicago <- chicago %>% mutate(salaries = if_else(
                              is.na(salaries), hours*hourly_rates*52, salaries),
                              emp_type = if_else(
                              ft_or_pt == 'F', 'Full Time', 'Part Time')) %>%
                       select(-ft_or_pt)

#similar analysis of departments by full_time and part time for this group
p <- ggplot(data=chicago %>% filter(emp_type == 'Full Time'), 
            aes(x=salaries)) +
      geom_density() + 
      theme_classic() +
      ggtitle('Density of Salaries in Chicago')
p

chic_sum <- chicago %>% group_by(departments, emp_type) %>% 
                summarise(`min` = min(salaries),
                          `25%` = quantile(salaries, 0.25),
                          `median` = quantile(salaries, 0.5),
                          `mean` = mean(salaries),
                          `75%` = quantile(salaries, 0.75),
                          `max` = max(salaries))
datatable(chic_sum) %>% 
    formatCurrency(c('min', '25%', 'mean', 'median',
                                          '75%', 'max'), digits=0)

```

#### Marin
Marin has no salary numbers and it spans multiple years of data. Let's examine only one year, to make it 1:1 with the other data sets in regards to counts. Looking at the year 2020, we'll examine the department by gender.
```{r marin_graph, fig.height=6, fig.width=12, echo=FALSE, warning=FALSE, message=FALSE}
temp <- marin %>% filter(genders=='Male' | genders=='Female')
ggplot(data=temp,
       aes(x=genders)) +
  geom_bar(stat='count', aes(fill=genders)) + 
  facet_wrap(vars(departments), scales='free') +
  ggtitle('Marin Departments by Gender')
```

### Chicago Salary Analysis
Here, we looked at 95% prediction interval ranges for these salaries by department to spot individual salaries out of the normal range. You can hover over a point to learn the person's name who corresponds to that point on the graph. 

To create this graph, I found the 95% prediction interval for each department's salary expectations and then joined that back onto the original data set. The SQL for this join would look something like this:
```
select chi.*
      ,mdl.*
from chicago as chi
left join model_data as mdl
on chi.departments = mdl.departments
```
That data can then be used to create the below jittered scatter plot, lattuced by department, with 95% prediction intervals for each department.

```{r chicago_money, echo=FALSE, warning=FALSE, message=FALSE}
temp <- chicago %>% filter(emp_type == 'Full Time')
glm_fit <- lm(salaries ~ 0 + departments, data=temp)
depts <- tibble(levels(chicago$departments))
colnames(depts) = c('departments')
preds <- data.frame(predict(glm_fit, newdata=depts, interval = "prediction"))
depts <- cbind(depts, preds)
```

```{r chicago_graph, fig.height=12, fig.width=10, echo=FALSE, warning=FALSE, message=FALSE}
chic_final <- left_join(chicago, depts, by = c('departments')) %>% 
  filter(emp_type == 'Full Time')
p <- ggplot(data=chic_final, aes(x=departments, y=salaries, text=names)) +
        geom_jitter() +
        facet_wrap(vars(departments), scale='free', ncol=4) +
        geom_hline(aes(yintercept=lwr)) + geom_hline(aes(yintercept=upr))
fig <- ggplotly(p, tooltip='names')
fig
```


